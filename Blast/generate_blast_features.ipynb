{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37e61d6-3748-4b74-8e68-4978a4cc04c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import click as ck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, Counter,OrderedDict,defaultdict\n",
    "import sys\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "tag_dict = {'biological_process':'bp','cellular_component':'cc','molecular_function':'mf'}\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a02bc5-92cc-490f-8adb-63429e7da8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Ontology(object):\n",
    "\n",
    "    def __init__(self, filename='data/go.obo', with_rels=False):\n",
    "        self.ont,self.alts = self.load(filename, with_rels)\n",
    "        self.ic = None\n",
    "\n",
    "    def has_term(self, term_id):\n",
    "        return term_id in self.ont\n",
    "\n",
    "    def calculate_ic(self, annots):\n",
    "        cnt = Counter()\n",
    "        for x in annots:\n",
    "            cnt.update(x)\n",
    "        self.ic = {}\n",
    "        for go_id, n in cnt.items():\n",
    "            parents = self.get_parents(go_id)\n",
    "            if len(parents) == 0:\n",
    "                min_n = n\n",
    "            else:\n",
    "                min_n = min([cnt[x] for x in parents])\n",
    "            self.ic[go_id] = math.log(min_n / n, 2)\n",
    "    \n",
    "    def get_ic(self, go_id):\n",
    "        if self.ic is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.ic:\n",
    "            return 0.0\n",
    "        return self.ic[go_id]\n",
    "\n",
    "    def load(self, filename, with_rels):\n",
    "        ont = dict()\n",
    "        alts = dict()\n",
    "        obj = None\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if line == '[Term]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = dict()\n",
    "                    obj['is_a'] = list()\n",
    "                    obj['part_of'] = list()\n",
    "                    obj['regulates'] = list()\n",
    "                    obj['alt_ids'] = list()\n",
    "                    obj['is_obsolete'] = False\n",
    "                    continue\n",
    "                elif line == '[Typedef]':\n",
    "                    obj = None\n",
    "                else:\n",
    "                    if obj is None:\n",
    "                        continue\n",
    "                    l = line.split(\": \")\n",
    "                    if l[0] == 'id':\n",
    "                        obj['id'] = l[1]\n",
    "                    elif l[0] == 'alt_id':\n",
    "                        obj['alt_ids'].append(l[1])\n",
    "                    elif l[0] == 'namespace':\n",
    "                        obj['namespace'] = l[1]\n",
    "                    elif l[0] == 'is_a':\n",
    "                        obj['is_a'].append(l[1].split(' ! ')[0])\n",
    "                    elif with_rels and l[0] == 'relationship':\n",
    "                        it = l[1].split()\n",
    "                        # add all types of relationships\n",
    "                        if it[0] == 'part_of':\n",
    "                            obj['is_a'].append(it[1])\n",
    "#                             obj['part_of'].append(it[1])\n",
    "                            \n",
    "                            \n",
    "                    elif l[0] == 'name':\n",
    "                        obj['name'] = l[1]\n",
    "                    elif l[0] == 'is_obsolete' and l[1] == 'true':\n",
    "                        obj['is_obsolete'] = True\n",
    "        if obj is not None:\n",
    "            ont[obj['id']] = obj\n",
    "        for term_id in list(ont.keys()):\n",
    "            for t_id in ont[term_id]['alt_ids']:\n",
    "                ont[t_id] = ont[term_id]\n",
    "            if ont[term_id]['is_obsolete']:\n",
    "                alts[term_id] = ont[term_id]\n",
    "                del ont[term_id]\n",
    "                \n",
    "        for term_id, val in ont.items():\n",
    "            if 'children' not in val:\n",
    "                val['children'] = set()\n",
    "            for p_id in val['is_a']:\n",
    "                if p_id in ont:\n",
    "                    if 'children' not in ont[p_id]:\n",
    "                        ont[p_id]['children'] = set()\n",
    "                    ont[p_id]['children'].add(term_id)\n",
    "        return ont,alts\n",
    "\n",
    "\n",
    "    def get_anchestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while(len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in self.ont[t_id]['is_a']:\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        return term_set\n",
    "\n",
    "\n",
    "    def get_parents(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        for parent_id in self.ont[term_id]['is_a']:\n",
    "            if parent_id in self.ont:\n",
    "                term_set.add(parent_id)\n",
    "        return term_set\n",
    "\n",
    "\n",
    "    def get_namespace_terms(self, namespace):\n",
    "        terms = set()\n",
    "        for go_id, obj in self.ont.items():\n",
    "            if obj['namespace'] == namespace:\n",
    "                terms.add(go_id)\n",
    "        return terms\n",
    "\n",
    "    def get_namespace(self, term_id):\n",
    "        return self.ont[term_id]['namespace']\n",
    "    \n",
    "    def get_term_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while len(q) > 0:\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for ch_id in self.ont[t_id]['children']:\n",
    "                    q.append(ch_id)\n",
    "        return term_set\n",
    "    \n",
    "def read_pkl(input_file):\n",
    "    with open(input_file,'rb') as fr:\n",
    "        temp_result = pkl.load(fr)\n",
    "    \n",
    "    return temp_result\n",
    "\n",
    "def save_pkl(output_file,data):\n",
    "    with open(output_file,'wb') as fw:\n",
    "        pkl.dump(data,fw)\n",
    "        \n",
    "def get_label(anations,func_list):\n",
    "    temp_result = []\n",
    "    for label in func_list:\n",
    "        if label in anations:\n",
    "            temp_result.append(1)\n",
    "        else:\n",
    "            temp_result.append(0)\n",
    "    return np.array(temp_result)\n",
    "\n",
    "# def getTag(go,annotation):\n",
    "#     alt_items = {'GO:0030819':'biological_process','GO:0030818':'biological_process','GO:0100036':'biological_process',\n",
    "#             'GO:1903474':'biological_process','GO:0008565':'molecular_function','GO:0000991':'molecular_function',\n",
    "#             'GO:0001029':'molecular_function','GO:0005430':'molecular_function','GO:0005395':'molecular_function',\n",
    "#             'GO:0001191':'molecular_function','GO:0005623':'cellular_component'}\n",
    "#     if annotation in alt_items:\n",
    "#         tags = tag_dict[alt_items[annotation]]\n",
    "#     else:\n",
    "#         tags = tag_dict[go.get_namespace(annotation)]\n",
    "        \n",
    "#     return tags\n",
    "\n",
    "def getTag(go,annotation):\n",
    "    \n",
    "    tags = tag_dict[go.get_namespace(annotation)]\n",
    "    return tags\n",
    "    \n",
    "go = Ontology('/home/wbshi/work/swissprot_data/train_test_data_handled_v4/handled_protein_messages/go.obo', with_rels=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499337ba-6de0-465d-8eb9-ff7e709a0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def read_fasta(filename):\n",
    "    seqs = list()\n",
    "    info = list()\n",
    "    seq = ''\n",
    "    inf = ''\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if seq != '':\n",
    "                    seqs.append(seq)\n",
    "                    info.append(inf)\n",
    "                    seq = ''\n",
    "                inf = line[1:]\n",
    "            else:\n",
    "                seq += line\n",
    "        seqs.append(seq)\n",
    "        info.append(inf)\n",
    "    return info, seqs\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ed5305-7da8-4a0a-be8b-b168397493a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_one_all_sequence done\n",
      "test_two_all_sequence done\n"
     ]
    }
   ],
   "source": [
    "#目前已经获取test数据集在train数据集上经过blast工具获取到相似蛋白，当前cell提取这些值\n",
    "# 其中，格式6、格式7、格式10、格式17的输出条目是可以修改的。输出格式选择 6 （--outfmt 6） ，\n",
    "# 6是tabular格式对应BLAST的m8格式;\n",
    "# 默认输出为：qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore 。\n",
    "# 这12列对应的信息分别是：\n",
    "\n",
    "# Query id：查询序列ID标识\n",
    "# Subject id：比对上的目标序列ID标识\n",
    "# % identity：序列比对的一致性百分比\n",
    "# alignment length：符合比对的比对区域的长度\n",
    "# mismatches：比对区域的错配数\n",
    "# gap openings：比对区域的gap数目\n",
    "# q. start：比对区域在查询序列(Query id)上的起始位点\n",
    "# q. end：比对区域在查询序列(Query id)上的终止位点\n",
    "# s. start：比对区域在目标序列(Subject id)上的起始位点\n",
    "# s. end：比对区域在目标序列(Subject id)上的终止位点\n",
    "# e-value：比对结果的期望值，将比对序列随机打乱重新组合，和数据库进行比对，如果功能越保守，则该值越低；\n",
    "#该E值越高说明比对的高得分值是由GC区域，重复序列导致的。对于判断同源性是非常有意义的几个参数。\n",
    "# bit score：比对结果的bit score值\n",
    "\n",
    "def read_blast_result(input_file,output_file):\n",
    "    \n",
    "    result_dict = {}\n",
    "    train_data = read_pkl('/home/wbshi/work/swissprot_data/train_test_data_handled_v4/train_data_separate.pkl')\n",
    "    \n",
    "    with open(input_file,'r') as fr:\n",
    "        for line in fr:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                line = line.split()\n",
    "                # line[0] = line[0].split('|')[1]\n",
    "                # line[1] = line[1].split('|')[1]\n",
    "\n",
    "                # print(line)\n",
    "                # sys.exit(0)\n",
    "                if line[1] not in train_data:\n",
    "                    continue\n",
    "                    \n",
    "                if line[0] not in result_dict:\n",
    "                    result_dict[line[0]] = OrderedDict()\n",
    "#                     result_dict[line[0]] = {}\n",
    "#                 if line[0] == line[1]:\n",
    "#                     continue\n",
    "#                 print(line)\n",
    "                if line[1] not in result_dict[line[0]]:\n",
    "                    result_dict[line[0]][line[1]] = []\n",
    "            \n",
    "#                 result_dict[line[0]].append((line[1],line[2:]))\n",
    "                result_dict[line[0]][line[1]].append(line[2:])\n",
    "                \n",
    "    save_pkl(output_file,result_dict)\n",
    "\n",
    "    \n",
    "output_path = './'\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "blast_result = './test_one_all_sequence.blast'\n",
    "output_file = output_path + 'test_one_all_sequence_result_score.pkl'\n",
    "\n",
    "read_blast_result(blast_result,output_file)\n",
    "\n",
    "print('test_one_all_sequence done')\n",
    "\n",
    "\n",
    "blast_result = './test_two_all_sequence.blast'\n",
    "output_file = output_path + 'test_two_all_sequence_result_score.pkl'\n",
    "\n",
    "read_blast_result(blast_result,output_file)\n",
    "\n",
    "print('test_two_all_sequence done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2d2f2a-86cd-4ff4-b4f5-5c231cf6cf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "test_one_all_sequence done\n",
      "2\n",
      "test_two_all_sequence done\n"
     ]
    }
   ],
   "source": [
    "#提取blast_knn的score值\n",
    "def comput_blast_knn(k,save_path, test_proteins):\n",
    "    test_scores = {}\n",
    "    for key,value in test_blast_data.items():\n",
    "        proteinId = key\n",
    "        \n",
    "        test_scores[proteinId] = {}\n",
    "        \n",
    "        all_count = 0\n",
    "        for i,(sim,score_list) in enumerate(value.items()):\n",
    "            if i >= k:\n",
    "                break\n",
    "            all_count += float(score_list[0][-1])\n",
    "\n",
    "        test_scores[proteinId]['sim_func'] = defaultdict(float)\n",
    "        test_scores[proteinId]['bp'] = defaultdict(float)\n",
    "        test_scores[proteinId]['cc'] = defaultdict(float)\n",
    "        test_scores[proteinId]['mf'] = defaultdict(float)\n",
    "        \n",
    "        for i,(sim,score_list) in enumerate(value.items()):\n",
    "            if i >= k:\n",
    "                break\n",
    "            \n",
    "            for func in list(train_data[sim]['all_bp'] | train_data[sim]['all_cc'] | train_data[sim]['all_mf']):\n",
    "                \n",
    "                test_scores[proteinId]['sim_func'][func] += float(score_list[0][-1])/all_count\n",
    "                inner_tag = tag_dict[go.get_namespace(func)]\n",
    "\n",
    "                test_scores[proteinId][inner_tag][func] += float(score_list[0][-1])/all_count\n",
    "    \n",
    "\n",
    "    \n",
    "    for p in test_proteins.keys():\n",
    "        if p not in test_scores:\n",
    "            test_scores[p] = {}\n",
    "            test_scores[p]['sim_func'] = {}\n",
    "            test_scores[p]['bp'] = {}\n",
    "            test_scores[p]['cc'] = {}\n",
    "            test_scores[p]['mf'] = {}\n",
    "\n",
    "    save_pkl(save_path.format(k),test_scores) \n",
    "\n",
    "if not os.path.exists(\"./predictions\"):\n",
    "    os.mkdir(\"./predictions\")\n",
    "\n",
    "input_file = './test_one_all_sequence_result_score.pkl'\n",
    "test_blast_data = read_pkl(input_file)\n",
    "train_data = read_pkl('/home/wbshi/work/swissprot_data/train_test_data_handled_v4/train_data_separate.pkl')\n",
    "save_path = './predictions/test_one_all_sequence_blast_knn_{0}_predict_score.pkl'\n",
    "\n",
    "test_proteins = read_pkl('/home/wbshi/work/swissprot_data/train_test_data_handled_v4/test_one_data_separate.pkl')\n",
    "for k in [10,20,30,40,50,50000]:\n",
    "    print(k)\n",
    "    comput_blast_knn(k,save_path, test_proteins)\n",
    "    \n",
    "print('test_one_all_sequence done')\n",
    "\n",
    "\n",
    "input_file = './test_two_all_sequence_result_score.pkl'\n",
    "test_blast_data = read_pkl(input_file)\n",
    "train_data = read_pkl('/home/wbshi/work/swissprot_data/train_test_data_handled_v4/train_data_separate.pkl')\n",
    "save_path = './predictions/test_two_all_sequence_blast_knn_{0}_predict_score.pkl'\n",
    "\n",
    "test_proteins = read_pkl('/home/wbshi/work/swissprot_data/train_test_data_handled_v4/test_two_data_separate.pkl')\n",
    "for k in [3,5,10,20,30,40,50,50000]:\n",
    "\n",
    "    print(k)\n",
    "    comput_blast_knn(k,save_path, test_proteins)\n",
    "    \n",
    "print('test_two_all_sequence done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepFRI",
   "language": "python",
   "name": "deepfri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
